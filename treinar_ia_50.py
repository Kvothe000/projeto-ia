# Binance/treinar_ia_50.py (UPGRADE GRADIENT BOOSTING)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier # <--- O NOVO MOTOR
from sklearn.utils import class_weight
import joblib

ARQUIVO = "dataset_50_coins_norm.csv"
ARQUIVO_MODELO = "modelo_ia_v5.pkl"

def treinar():
    print("ðŸ§  Carregando CÃ©rebro GBT (50 Moedas)...")
    try:
        df = pd.read_csv(ARQUIVO)
    except:
        print("âŒ Rode o gerar_dataset_50.py primeiro!")
        return

    print(f"ðŸ“š Estudando {len(df)} candles...")
    
    X = df.drop(columns=['target'])
    y = df['target']
    
    # DivisÃ£o
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

    # Calcular pesos (para equilibrar classes desbalanceadas)
    # Gradient Boosting nÃ£o tem parÃ¢metro class_weight automÃ¡tico no sklearn,
    # entÃ£o usamos sample_weight na hora do fit
    classes_weights = class_weight.compute_sample_weight(
        class_weight='balanced',
        y=y_train
    )

    print("ðŸ‹ï¸ Treinando Gradient Boosting (Isso Ã© rÃ¡pido!)...")
    
    # HistGradientBoostingClassifier Ã© inspirado no LightGBM (Muito rÃ¡pido e preciso)
    modelo = HistGradientBoostingClassifier(
        learning_rate=0.1,
        max_iter=300,           # Equivalente a n_estimators
        max_depth=12,
        min_samples_leaf=50,    # Mais conservador
        random_state=42,
        early_stopping=True     # Para de treinar se nÃ£o melhorar (evita overfitting)
    )
    
    modelo.fit(X_train, y_train, sample_weight=classes_weights)
    
    print("\nðŸ” Teste de PrecisÃ£o:")
    probs = modelo.predict_proba(X_test)
    
    # RelatÃ³rio Detalhado
    for i, nome in enumerate(['NEUTRO', 'LONG', 'SHORT']):
        if nome == 'NEUTRO': continue
        
        # Testa vÃ¡rios nÃ­veis de confianÃ§a
        for threshold in [0.55, 0.60, 0.70]:
            mask = probs[:, i] > threshold
            total = mask.sum()
            if total > 0:
                acertos = (y_test[mask] == i).sum()
                acc = acertos / total
                print(f"{nome} (> {threshold*100:.0f}%): {total} Trades -> {acc*100:.1f}% Acerto")
            else:
                print(f"{nome} (> {threshold*100:.0f}%): 0 Trades")

    joblib.dump(modelo, ARQUIVO_MODELO)
    print(f"\nðŸ’¾ CÃ©rebro Salvo: {ARQUIVO_MODELO}")

if __name__ == "__main__":
    treinar()