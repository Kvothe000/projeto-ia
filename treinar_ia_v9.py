# Binance/treinar_ia_v9.py - GBT + REGULARIZA√á√ÉO + EARLY STOPPING
import pandas as pd
import numpy as np
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.utils import class_weight
from sklearn.metrics import classification_report
import joblib

# Configura√ß√µes
ARQUIVO_DADOS = "dataset_v8_atr.csv" # Usamos o dataset V8 que j√° est√° bom (ATR Manual)
ARQUIVO_MODELO = "modelo_ia_v6.pkl"  # Sobrescreve para o bot usar direto

def treinar():
    print("üß† INICIANDO TREINO V9 (O Protocolo do Colega)...")
    
    try:
        df = pd.read_csv(ARQUIVO_DADOS)
    except:
        print("‚ùå Erro: Dataset n√£o encontrado.")
        return

    print(f"üìö Dados Carregados: {len(df)} candles.")
    
    # Separa Features e Alvo
    X = df.drop(columns=['target'])
    y = df['target']
    
    # 1. DIVIS√ÉO TEMPORAL (Sem Shuffle - Respeita o Tempo)
    # 80% Passado (Treino) | 20% Futuro (Teste)
    corte = int(len(df) * 0.80)
    
    X_train = X.iloc[:corte]
    y_train = y.iloc[:corte]
    
    X_test = X.iloc[corte:]
    y_test = y.iloc[corte:]
    
    print(f"‚è≥ Treino: {len(X_train)} | Valida√ß√£o Futura: {len(X_test)}")

    # 2. BALANCEAMENTO DE CLASSES
    # Calcula pesos para a IA dar a mesma import√¢ncia a Long/Short/Neutro
    sample_weights = class_weight.compute_sample_weight(
        class_weight='balanced', 
        y=y_train
    )

    # 3. MODELO V9 (HI-TECH)
    # Implementa as sugest√µes do colega: Regulariza√ß√£o e Early Stopping
    print("üèãÔ∏è Treinando Gradient Boosting com Regulariza√ß√£o L2...")
    
    modelo = HistGradientBoostingClassifier(
        learning_rate=0.05,      # Aprende devagar para n√£o decorar (Overfitting)
        max_iter=2000,           # Muitas itera√ß√µes permitidas...
        max_depth=8,             # √Årvores menos profundas (Mais generalistas)
        min_samples_leaf=100,    # Exige confirma√ß√£o forte de padr√£o
        l2_regularization=5.0,   # <--- A M√ÅGICA (Evita decorar ru√≠do)
        early_stopping=True,     # <--- A M√ÅGICA (Para se parar de melhorar)
        validation_fraction=0.1, # Usa 10% do treino para saber quando parar
        n_iter_no_change=20,     # Se n√£o melhorar em 20 rodadas, para.
        random_state=42
    )
    
    modelo.fit(X_train, y_train, sample_weight=sample_weights)
    
    # 4. AVALIA√á√ÉO DE ELITE
    print("\nüîç RESULTADO NO FUTURO (O Teste da Verdade):")
    probs = modelo.predict_proba(X_test)
    
    # Loop de Confian√ßa
    melhor_precisao = 0
    
    for i, nome in enumerate(['NEUTRO', 'LONG', 'SHORT']):
        if nome == 'NEUTRO': continue
        
        print(f"\n--- AN√ÅLISE {nome} ---")
        for conf in [0.55, 0.60, 0.70]:
            mask = probs[:, i] > conf
            total_trades = mask.sum()
            
            if total_trades > 0:
                acertos = (y_test[mask] == i).sum()
                acc = acertos / total_trades
                print(f"Confian√ßa > {conf*100:.0f}%: {total_trades} Trades -> {acc*100:.1f}% Acerto")
                if total_trades > 10 and acc > melhor_precisao:
                    melhor_precisao = acc
            else:
                print(f"Confian√ßa > {conf*100:.0f}%: 0 Trades")

    # 5. SALVAR
    joblib.dump(modelo, ARQUIVO_MODELO)
    print(f"\nüíæ Modelo Salvo! (Precis√£o Base: {modelo.score(X_test, y_test)*100:.1f}%)")

if __name__ == "__main__":
    treinar()